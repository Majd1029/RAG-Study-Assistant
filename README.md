# ðŸ“š Advanced RAG Study Assistant

An offline **Retrieval-Augmented Generation (RAG)** study assistant that helps you query multiple PDFs and get context-aware answers with citations.  
It includes a **Streamlit frontend** for interactive use and a **FastAPI backend** for programmatic access. Built with **Ollama Mistral**, **FAISS**, and **LangChain**.

---

## **Architecture**

![RAG Study Assistant Architecture](RAG_Architecture.png)

The diagram above illustrates the complete data flow:
1. **PDF Ingestion**: Documents are processed and chunked via `ingest.py`
2. **Vector Database**: Text chunks are embedded and stored in FAISS index
3. **RAG Pipeline**: User queries are embedded and matched against relevant document chunks
4. **LLM Generation**: Ollama Mistral generates answers with source citations
5. **Chat Memory**: Conversation history is maintained for multi-turn interactions

---

## **Features**

- Multi-PDF ingestion with metadata tracking
- Semantic search using **FAISS** + embeddings (`all-mpnet-base-v2`)
- Answers generated by **Ollama Mistral** LLM
- Multi-turn **chat memory** (previous questions/answers remembered)
- Highlighted source citations in answers `(PDF: ..., Page X)`
- Optional **Summarization** and **Quiz generation** endpoints (extensible)
- Streamlit UI + FastAPI backend for maximum flexibility
- Fully offline and local

---

## **Project Structure**
```bash
rag_study_advanced/
â”‚
â”œâ”€â”€ app.py # Streamlit frontend
â”œâ”€â”€ api.py # FastAPI backend
â”œâ”€â”€ ingest.py # PDF ingestion + FAISS
â”œâ”€â”€ rag_pipeline.py # Retrieval + LLM logic
â”œâ”€â”€ chat_memory.json # Multi-turn chat memory
â”œâ”€â”€ faiss_index/ # FAISS vector DB
â””â”€â”€ requirements.txt 
```

## **Key dependencies:**

- streamlit
- fastapi, uvicorn
- langchain, langchain-community, langchain-text-splitters
- sentence-transformers
- transformers
- faiss-cpu
- pypdf
- torch, torchvision, torchaudio
---

## **Installation**

1. Clone the repository:

```bash
git clone <your-repo-url>
cd rag_study_advanced
```

2. Create and activate a Python virtual environment:
PowerShell:

```bash
python -m venv rag_env
.\rag_env\Scripts\Activate.ps1
```

CMD:
```bash
python -m venv rag_env
rag_env\Scripts\activate.bat
```
3. Install dependencies:

```bash
pip install -r requirements.txt
```
##  Usage
**Run Streamlit Frontend**
```bash
streamlit run app.py
```
- Open in browser: http://localhost:8501
- Upload PDFs
- Ask questions interactively
- Reset conversation memory if needed

**Run FastAPI Backend**
```bash
uvicorn api:app --reload
```

- API Docs (Swagger UI): http://127.0.0.1:8000/docs

- Endpoints:

    - /upload_pdfs/ â†’ Upload PDFs to build FAISS index

    - /ask/ â†’ Ask questions programmatically

- Supports multi-turn chat memory

## Example API Usage (cURL)

**Upload PDFs:**
```bash
curl -F "files=@notes1.pdf" -F "files=@notes2.pdf" http://127.0.0.1:8000/upload_pdfs/
```
```bash
curl -X POST -F "question=Explain photosynthesis" http://127.0.0.1:8000/ask/
```
```bash
{
  "question": "Explain photosynthesis",
  "answer": "(PDF: BiologyNotes.pdf | Page 12) Photosynthesis is the process..."
}
```

## **âš¡ Notes**

- Chat memory stored in chat_memory.json.
- Answers include PDF filename and page number for citations.
- FAISS vector store is saved locally in faiss_index/.
- Ollama Mistral can run locally offline (make sure itâ€™s installed and configured).

## **ðŸ“ƒ License**

MIT License â€” free for academic, personal, and portfolio use.